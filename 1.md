# Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images

## Summary
This paper shows that even SOTA deep neural networks produce a prediction with high confidence score on a image that is unrecognizable to human. 

They use evolutionary algorithms (EA) to generate fooling images. There are two EA approaches (direct and indirect - CPPN).

This paper reveals some interesting points about the DNN: 

1) Images classified as a 1 tend to have vertical bars, while images classified as a 2 tend to have a horizontal bar in the lower half of the image. Qualitatively similar discriminative features
are observed in 50 other runs as well (supplementary material). This result suggests that the EA exploits specific discriminative features corresponding to the handwritten digits learned by MNIST DNNs. 

2) From 1, they hypothesized that MNIST DNNs might be easily
fooled because they are trained on a small dataset that could allow for overfitting (MNIST has only 60,000 training images). To test this hypothesis that a larger dataset might prevent the pathology, we evolved directly encoded images to be classified confidently by a convolutional DNN [16] trained on the ImageNet 2012 dataset, which has 1.3 million natural images in 1000 classes. The directly encoded EA was less successful at producing high-confidence images in this case. 

3) Once again, now they test using images from indirect encoding (CPPN). These results suggest that DNNs tend to learn low and middle-level features rather than the global structure of
objects. If DNNs were properly learning global structure, images should receive lower DNN confidence scores if they
contain repetitions of object subcomponents that rarely appear in natural images, such as many pairs of fox ears or
endless remote buttons. For example, instead of distinguishing cat and duck by their shapes, DNNS tend to classify based on tails, ears or even eyes.

4) They seek the answer for the question: if images that fool one DNN also fool another or whether different
DNNs learn the same features for each class, or whether each trained DNN learns different discriminative features.

5) They add negative examples to the training set and retrain the model and hope that those newly added samples will be classified to ``fooling`` class.

6) They introduce one more method to generate images fooling DNNs using an interpretable ML technique. This technique calculate the gradient of the posterior prob. for a specific class, then increase the activation for chosen units. This helps find images that produce high confidence classifications. Using L2 regularization, the generated images are somehow visually recognizable.
