# InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets

## Summary
This paper proposes to decompose to latent code of the vanilla GANs into an incompressible noise z; and c which controls the semantic features of the synthetic images (e.g. control angle and thickness of digit strokes).

The Generator now is G(z,c). However, standard GANs dont know the c, so to integrate this component into the GAN, they propose an **information-theoretic** regularization between the latent codes c and generator distribution G(z,c); I(c;G(z,c)). 

Mutual information between X and Y.
* I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)

while H is the entropy of the distribution.

## Experiments
In addition to control the shape of MNIST digits, they also makes use of c2 (angle) and c3 (thickness) as continuous variables.
