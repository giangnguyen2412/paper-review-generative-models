# Least Squares Generative Adversarial Networks

## Summary 
Their approach was based on the observation of the limitations for using binary cross entropy loss 
when generated images are very different from real images, which can lead to very small or vanishing gradients, 
and in turn, little or no update to the model.

The discriminator seeks to minimize the sum squared difference between predicted and expected values for real and fake images.

discriminator: minimize (D(x) – 1)^2 + (D(G(z)))^2
The generator seeks to minimize the sum squared difference between predicted and expected values as though the generated images were real.

generator: minimize (D(G(z)) – 1)^2
In practice, this involves maintaining the class labels of 0 and 1 for fake and real images respectively, minimizing the least squares, 
also called mean squared error or L2 loss.

l2 loss = sum (y_predicted – y_true)^2
The benefit of the least squares loss is that it gives more penalty to larger errors, in turn resulting in a large correction rather 
than a vanishing gradient and no model update.
